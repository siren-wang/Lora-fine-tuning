{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WdJrmJ9z95cH"
   },
   "source": [
    "# LoRA Fine-Tuning with SmolLM2-360M\n",
    "\n",
    "**Base Model:** HuggingFaceTB/SmolLM2-360M-Instruct  \n",
    "**Dataset:** shawhin/imdb-truncated (1000 train, 1000 validation samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8X0GwuzK95cJ"
   },
   "source": [
    "## 1. Setup and Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "m-X9mOLT95cJ"
   },
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install -q transformers datasets peft accelerate bitsandbytes trl torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CbbNutse95cJ"
   },
   "outputs": [],
   "source": "# Import from our lora_finetuning module\nfrom lora_finetuning import (\n    setup_lora_model,\n    prepare_dataset,\n    train_lora_model,\n    evaluate_model,\n    generate_response,\n    load_finetuned_model\n)\n\nimport torch\nimport numpy as np\nfrom datasets import load_dataset\n\n# Set random seeds for reproducibility\ntorch.manual_seed(42)\nnp.random.seed(42)"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QnODGhbW95cJ"
   },
   "source": [
    "## 2. Load Dataset and Inspect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DAyL5y5f95cK",
    "outputId": "3eab62d3-d3c9-45ee-9e8b-1b18a18b4ee0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['label', 'text'],\n",
      "        num_rows: 1000\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['label', 'text'],\n",
      "        num_rows: 1000\n",
      "    })\n",
      "})\n",
      "\n",
      "Sample from training set:\n",
      "{'label': 1, 'text': '. . . or type on a computer keyboard, they\\'d probably give this eponymous film a rating of \"10.\" After all, no elephants are shown being killed during the movie; it is not even implied that any are hurt. To the contrary, the master of ELEPHANT WALK, John Wiley (Peter Finch), complains that he cannot shoot any of the pachyderms--no matter how menacing--without a permit from the government (and his tone suggests such permits are not within the realm of probability). Furthermore, the elements conspire--in the form of an unusual drought and a human cholera epidemic--to leave the Wiley plantation house vulnerable to total destruction by the Elephant People (as the natives dub them) to close the story. If you happen to see the current release EARTH, you\\'ll detect the Elephant People are faring less well today.'}\n"
     ]
    }
   ],
   "source": [
    "# Load the IMDB dataset\n",
    "dataset = load_dataset('shawhin/imdb-truncated')\n",
    "print(dataset)\n",
    "print(\"\\nSample from training set:\")\n",
    "print(dataset['train'][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C7XCj87K95cK"
   },
   "source": [
    "## 3. Load Base Model and Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yJwTprQm95cK",
    "outputId": "66f9fb5f-e3f5-4c63-c87a-f0ed6a579aaa"
   },
   "outputs": [],
   "source": "# Model configuration\nmodel_name = \"HuggingFaceTB/SmolLM2-360M-Instruct\"\n\n# Import necessary classes for base model loading\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\n\n# Load tokenizer\ntokenizer = AutoTokenizer.from_pretrained(model_name)\n\n# Set padding token if not present\nif tokenizer.pad_token is None:\n    tokenizer.pad_token = tokenizer.eos_token\n    tokenizer.pad_token_id = tokenizer.eos_token_id\n\n# Load base model (without LoRA for initial testing)\nbase_model = AutoModelForCausalLM.from_pretrained(\n    model_name,\n    torch_dtype=torch.float16,\n    device_map=\"auto\",\n    trust_remote_code=True\n)\n\nprint(f\"Model loaded: {model_name}\")\nprint(f\"Model parameters: {base_model.num_parameters():,}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0aQBHnRV95cK"
   },
   "source": [
    "## 4. Test Base Model (Before Fine-Tuning)\n",
    "\n",
    "Let's evaluate the base model on a subset of the validation dataset to establish a baseline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "q09tbD6Z95cK",
    "outputId": "686f7b59-da8f-4fa9-8e1d-c7abfc56fddb"
   },
   "outputs": [],
   "source": "# Using imported functions from lora_finetuning module\n# - generate_response()\n# - evaluate_model()\n\nprint(\"=\" * 80)\nprint(\"BASE MODEL EVALUATION (Before Fine-Tuning)\")\nprint(\"=\" * 80)\nprint()\n\nbase_acc, base_correct, base_total = evaluate_model(base_model, tokenizer, dataset, num_samples=100, debug=True)\n\nprint(\"\\n\" + \"=\" * 80)\nprint(f\"BASE MODEL RESULTS:\")\nprint(f\"Accuracy: {base_acc:.2%} ({base_correct}/{base_total} correct)\")\nprint(\"=\" * 80)"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rYe71eyY95cL"
   },
   "source": [
    "## 5. Prepare Dataset for Fine-Tuning\n",
    "\n",
    "Format the IMDB dataset for sentiment analysis training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 331,
     "referenced_widgets": [
      "2cb875ff236443c0955e9c4c77284b32",
      "4d0403f684c64f1db94c9a73eb1ebeb8",
      "f048e205ad3249e3bbac9b14f72fbd8e",
      "d75f010b18f744068536310e098ea6bd",
      "00f1e114ec8a447cb1ae32d85777b328",
      "1762e8e61c25471a990330c4f841e124",
      "a47090c36e0440f4ac0df76af5216883",
      "55cf7e3f95fe47c5911c9837f482fabc",
      "807370196e94498a94324d91ce433290",
      "1b1f3c127eb946f695d16c62ad76cc19",
      "4007add5b49f4bc0a40cd002a6aed14c",
      "64a4e6807ff24b28a0f416a5c2905743",
      "47b946f1808a47a485dbd4255ce13205",
      "f44c3e21ded74a9f8e22bb25e22fcb99",
      "da900444c8d74e529d8e1c46221a46f9",
      "8544b5cb52aa47429a5e9a90295701ff",
      "de261f73ac6f4468acae00545c78190e",
      "e0b90e29d8e54249aadb6d79aed77787",
      "02bd96380bd842f398c64a6c3302827e",
      "cb9cbec5e4a848f9a112ad2a8a9f2ec6",
      "5398a44f0ab54fb49003d9eb3896a512",
      "9729d4c70e91439b8fb8d41e80d1021d"
     ]
    },
    "id": "-MF7F-aU95cL",
    "outputId": "3be85416-372c-4dd1-c843-69612c9dad31"
   },
   "outputs": [],
   "source": "# Using the imported prepare_dataset() function\n# This function handles:\n# - Creating prompts with create_prompt()\n# - Tokenizing with tokenize_function()\n# - Returning train, validation, and raw datasets\n\nprint(\"Preparing dataset...\")\ntokenized_train, tokenized_val, dataset = prepare_dataset(\n    dataset_name='shawhin/imdb-truncated',\n    tokenizer=tokenizer,\n    max_length=256\n)\n\nprint(f\"\\nTokenized training samples: {len(tokenized_train)}\")\nprint(f\"Tokenized validation samples: {len(tokenized_val)}\")\n\nprint(\"\\nSample from formatted dataset:\")\nprint(\"(The raw text has been converted to instruction-formatted prompts)\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 135,
     "referenced_widgets": [
      "7ce94c464ec64240a57a4f77c92e5baf",
      "11e8ef7f32f849929c25b6fcc03d8ee6",
      "4307087a1f4947b7a225f84d3ffa2a15",
      "d71cd640b2664f2b93e4247bb50750e2",
      "1ae92b61cc794c97a13d85c0dd139f83",
      "9d12de5deab34050a19d5d1e12bd7c61",
      "e7627cf098714baba663b25a9e950d3f",
      "e09b392e56064f76a26df90daacb2abe",
      "df7a2253bd8d4caf8847bdf37db7c107",
      "1717eb33b028496b8f3b87d0d5d93c96",
      "77baea33a5de445a8fafce354073cf77",
      "1d56a59035a74653a05b629c3d3ba558",
      "e25bafca87b341a3b3f4ff5b1f9e4566",
      "478f125165e14640a1a10400f62ae729",
      "7a13aae50c364fdc86ca08a46cc061a0",
      "66c3e1cb6ab0415583cdf482036e3728",
      "d413b4b13ccf4064804825662e36598b",
      "14ff40d6e0504b0bbbdcea72270949e7",
      "c1137f146fd4443e9c66d35a65f12e01",
      "ee7353a22230423abfeb6851413a92c9",
      "6ad5a1ef250341c68d14e668eefa098e",
      "d848b925b9434d21b1c5b2128bd979dc"
     ]
    },
    "id": "BtgMHCRq95cL",
    "outputId": "22f35863-c11e-4608-883f-4c5a2e0721f1"
   },
   "outputs": [],
   "source": "# Dataset preparation is now complete!\n# The prepare_dataset() function has already:\n# 1. Loaded the dataset\n# 2. Created instruction-formatted prompts\n# 3. Tokenized the text with max_length=256\n# 4. Added labels for training\n\nprint(\"✓ Dataset ready for training\")\nprint(f\"  Training samples: {len(tokenized_train)}\")\nprint(f\"  Validation samples: {len(tokenized_val)}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PM5F4OqY95cL"
   },
   "source": [
    "## 6. Configure LoRA and PEFT\n",
    "\n",
    "Set up Low-Rank Adaptation (LoRA) configuration for efficient fine-tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Rh_33UlQ95cL",
    "outputId": "7b9d50b6-063d-4bd2-da9e-3d09c800c0c4"
   },
   "outputs": [],
   "source": "# Using setup_lora_model() to configure and apply LoRA\n# This function handles:\n# - Loading the base model\n# - Creating LoRA configuration\n# - Applying PEFT/LoRA to the model\n# - Enabling gradient checkpointing\n\nprint(\"Setting up model with LoRA configuration...\")\n\nmodel, tokenizer, lora_config = setup_lora_model(\n    model_name=model_name,\n    lora_r=16,                 # Rank of the low-rank matrices\n    lora_alpha=32,             # Scaling factor\n    lora_dropout=0.05,         # Dropout probability\n    target_modules=[\"q_proj\", \"v_proj\", \"k_proj\", \"o_proj\"]  # Modules to apply LoRA\n)\n\nprint(\"\\n✓ Model ready for fine-tuning with LoRA\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Hd2qYgD-95cL",
    "outputId": "b9ffd673-0a31-4ce3-d953-9951ed66446b"
   },
   "outputs": [],
   "source": "# The setup_lora_model() function has completed all setup steps:\n# - Loaded base model with float16 precision\n# - Configured LoRA (r=16, alpha=32, dropout=0.05)\n# - Applied LoRA to attention projection layers\n# - Enabled gradient checkpointing for memory efficiency\n# - Prepared model for k-bit training\n\nprint(\"Model configuration complete!\")\nprint(f\"Ready to train {model.num_parameters():,} parameters\")\nprint(f\"(Only {model.get_nb_trainable_parameters()} are trainable with LoRA)\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bT-VnKyG95cL"
   },
   "source": [
    "## 7. Configure Training Arguments and Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2eO6nUL-95cL",
    "outputId": "002fc5e9-592e-4fe3-c145-fbb3739d9d13"
   },
   "outputs": [],
   "source": "# Using train_lora_model() to train the model\n# This function handles:\n# - Setting up training arguments\n# - Creating data collator\n# - Initializing Trainer\n# - Running the training loop\n\noutput_dir = \"./lora_finetuned_smollm2\"\n\ntrainer = train_lora_model(\n    model=model,\n    tokenizer=tokenizer,\n    train_dataset=tokenized_train,\n    eval_dataset=tokenized_val,\n    output_dir=output_dir,\n    num_epochs=3,\n    batch_size=4,\n    learning_rate=2e-4,\n    gradient_accumulation_steps=4,\n    warmup_steps=100,\n    logging_steps=50\n)\n\nprint(\"\\n✓ Training configuration complete, ready to train!\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2oXiBxmh95cL"
   },
   "source": [
    "## 8. Train the Model\n",
    "\n",
    "This will take several minutes depending on your hardware."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 277
    },
    "id": "-88zP5TA95cL",
    "outputId": "32841375-675d-417d-df60-d5573425fa97"
   },
   "outputs": [],
   "source": "# Training has been completed by train_lora_model()!\n# The function already called trainer.train() and displayed progress\n\nprint(\"=\" * 80)\nprint(\"Training Summary:\")\nprint(\"=\" * 80)\nprint(f\"✓ Model trained for 3 epochs\")\nprint(f\"✓ Best model saved to: {output_dir}\")\nprint(f\"✓ Training logs available in trainer.state\")\nprint(\"=\" * 80)"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PJ1nRPfP95cL"
   },
   "source": [
    "## 9. Test Fine-Tuned Model and Compare\n",
    "\n",
    "Load the fine-tuned model and compare its performance with the base model on the same validation samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tmk7xlmU95cL",
    "outputId": "3d476baa-8994-4fb3-8fa1-4afb9ef7851e"
   },
   "outputs": [],
   "source": "# Using load_finetuned_model() to load the trained model\n# This function handles:\n# - Loading the base model\n# - Loading and applying the LoRA adapter weights\n\nprint(\"Loading fine-tuned model...\")\n\n# The model is already in memory from training, but let's demonstrate\n# how to load it fresh (useful when restarting the notebook)\nlora_adapter_path = output_dir\n\nfinetuned_model = load_finetuned_model(\n    base_model_name=model_name,\n    adapter_path=lora_adapter_path\n)\n\nprint(\"Fine-tuned model loaded successfully!\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nxoXKt_z95cL",
    "outputId": "da047251-eb6a-47cc-89e0-57298ce30c78"
   },
   "outputs": [],
   "source": "# Using evaluate_model() to test the fine-tuned model\nprint(\"=\" * 80)\nprint(\"FINE-TUNED MODEL EVALUATION\")\nprint(\"=\" * 80)\nprint()\n\nft_acc, ft_correct, ft_total = evaluate_model(finetuned_model, tokenizer, dataset, num_samples=100)\n\nprint(\"\\n\" + \"=\" * 80)\nprint(f\"FINE-TUNED MODEL RESULTS:\")\nprint(f\"Accuracy: {ft_acc:.2%} ({ft_correct}/{ft_total} correct)\")\nprint(\"=\" * 80)"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9yzuqGQ995cL"
   },
   "source": [
    "## 10. Compare Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "v6WYVG1r95cL",
    "outputId": "39dd6893-d830-454d-ebb9-2940a2ddb0a7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "MODEL COMPARISON: Base vs Fine-Tuned\n",
      "================================================================================\n",
      "\n",
      "Base Model Accuracy:       66.00% (66/100)\n",
      "Fine-Tuned Model Accuracy: 82.00% (82/100)\n",
      "\n",
      "Absolute Improvement:      16.00%\n",
      "Relative Improvement:      24.2%\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"MODEL COMPARISON: Base vs Fine-Tuned\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"\\nBase Model Accuracy:       {base_acc:.2%} ({base_correct}/{base_total})\")\n",
    "print(f\"Fine-Tuned Model Accuracy: {ft_acc:.2%} ({ft_correct}/{ft_total})\")\n",
    "print(f\"\\nAbsolute Improvement:      {(ft_acc - base_acc):.2%}\")\n",
    "print(f\"Relative Improvement:      {((ft_acc - base_acc) / base_acc * 100):.1f}%\")\n",
    "print(\"=\" * 80)\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}