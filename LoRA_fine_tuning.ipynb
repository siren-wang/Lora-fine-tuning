{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WdJrmJ9z95cH"
      },
      "source": [
        "# LoRA Fine-Tuning with SmolLM2-360M\n",
        "\n",
        "**Base Model:** HuggingFaceTB/SmolLM2-360M-Instruct  \n",
        "**Dataset:** shawhin/imdb-truncated (1000 train, 1000 validation samples)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 74
        },
        "id": "ggc480smbNfn",
        "outputId": "b3170393-fc24-4abc-f1c7-3d4effccae34"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-1847f779-258a-457d-bae4-0e9d2eb52675\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-1847f779-258a-457d-bae4-0e9d2eb52675\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving lora_finetuning.py to lora_finetuning (1).py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8X0GwuzK95cJ"
      },
      "source": [
        "## 1. Setup and Installation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "m-X9mOLT95cJ"
      },
      "outputs": [],
      "source": [
        "# Install required packages\n",
        "!pip install -q transformers datasets peft accelerate bitsandbytes trl torch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "CbbNutse95cJ"
      },
      "outputs": [],
      "source": [
        "# Import from our lora_finetuning module\n",
        "from lora_finetuning import (\n",
        "    setup_lora_model,\n",
        "    prepare_dataset,\n",
        "    train_lora_model,\n",
        "    evaluate_model,\n",
        "    generate_response,\n",
        "    load_finetuned_model\n",
        ")\n",
        "\n",
        "import torch\n",
        "import numpy as np\n",
        "from datasets import load_dataset\n",
        "\n",
        "# Set random seeds for reproducibility\n",
        "torch.manual_seed(42)\n",
        "np.random.seed(42)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QnODGhbW95cJ"
      },
      "source": [
        "## 2. Load Dataset and Inspect"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 536,
          "referenced_widgets": [
            "8fb029279ebb4b078c0fe2561552dbb8",
            "dec3e3f79dda4e478f0eeb3afecd1de7",
            "fe2fdfbf63ed4e03819c13de716662e5",
            "75331504ce114118b79df8560c514925",
            "80dfcf9a835a4c608b9fb40bd9c90135",
            "322520937dc6440eb6b8344092e1faa1",
            "398274a892244c4382ef8d153fe3eb44",
            "f75ee28d8b944f0d973f4976f7523b41",
            "754ccc03f08f4c849eb6b0f3e52ee1e2",
            "12df9b5f932e427d9b0b3a88c06f3489",
            "d6ab6b317ecb4cd4bf4b0eaf22408d06",
            "9c50dc02a557472fbc00aa8fc619afe5",
            "6d8bc7f03d664b83870fa2a2951a95a8",
            "d7ad94427e4845dd92686e451abcbc35",
            "b94a5eff2fc24e15ba1b7ff4ca25debc",
            "ec21e9ed06ab48a2949b81b65cb73212",
            "436576105cd54f84a1d096617d71e0b3",
            "53b119d774d64c7b9bff0f9d63e8b023",
            "04c514a4fe7940dab6a8dcc848e672c5",
            "bc5195df8a48488284b69e79372619db",
            "b8d726e86ae94c16a731f2abb751e591",
            "e48f90e058ca4a578408a4354c229de8",
            "3aad7f4b00e74f9f866b628a21d99c58",
            "bc41b759680c4867acf0767b2364cf83",
            "dbc2f8a1377b46ff97703d07e382f72f",
            "ce0fcd480e4d4d27b8142bd3fc367eac",
            "0aeaf196eda743e5918755746c220ea9",
            "b7feb1f21a224de89fd078aac7b0b945",
            "c86298c724a348efabb867840feca6b2",
            "939a1057ebf843a8870c6db47eea3304",
            "3fb7c7eed0754b2d99cd475834928c04",
            "df1d4e51de7a48b5bcc7af6501dad6a5",
            "6c0ac8c12b614aa98f40d910f4e50599",
            "ee0795cb6ffb430fa457ed3e25149410",
            "3af9930fae3d452f88783d2a194708df",
            "b688443df831499d825db673719afd91",
            "e519ed5d2ca94cc29d68c2d139c1b4b1",
            "68590f777db143d9b8ba3f1173172c88",
            "024315c624844486922ee16edf492fd1",
            "1a8294ad01ae40ce8e0696c6842e9953",
            "0799252f0348403d928789a798c7958b",
            "20fa1b17f50143d59295f5b94473a2f2",
            "318ef1ee00dd49b4989b63f2305179ae",
            "9400f913329d4201b564e571b02e13d4",
            "e1a18168ffbf44fd86e7a05ed539737f",
            "61aa665d79a645f583f07fd6eaf2483d",
            "735e129fa46c430a8944715e58cedc30",
            "d261bae2019a46d88bf837db2abd9eef",
            "330514a0be23475598af585136daf3ce",
            "f677387f260145eb8996234948e09740",
            "3d230182a390467c987be33afd3610f2",
            "0f6c8a9d89fa44d591fdb9f5895bdd3d",
            "274cab6d04d84d57862a63e173f3889d",
            "bcc34c013f354cacb213c34f3e433a08",
            "c79e213285104221990029bb66cea927"
          ]
        },
        "id": "DAyL5y5f95cK",
        "outputId": "d8ab98dd-e492-45dc-ee17-8cd295c95f48"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "README.md:   0%|          | 0.00/592 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "8fb029279ebb4b078c0fe2561552dbb8"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "data/train-00000-of-00001-5a744bf76a1d84(…):   0%|          | 0.00/836k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "9c50dc02a557472fbc00aa8fc619afe5"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "data/validation-00000-of-00001-a3a52fabb(…):   0%|          | 0.00/853k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "3aad7f4b00e74f9f866b628a21d99c58"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Generating train split:   0%|          | 0/1000 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ee0795cb6ffb430fa457ed3e25149410"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Generating validation split:   0%|          | 0/1000 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "e1a18168ffbf44fd86e7a05ed539737f"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DatasetDict({\n",
            "    train: Dataset({\n",
            "        features: ['label', 'text'],\n",
            "        num_rows: 1000\n",
            "    })\n",
            "    validation: Dataset({\n",
            "        features: ['label', 'text'],\n",
            "        num_rows: 1000\n",
            "    })\n",
            "})\n",
            "\n",
            "Sample from training set:\n",
            "{'label': 1, 'text': '. . . or type on a computer keyboard, they\\'d probably give this eponymous film a rating of \"10.\" After all, no elephants are shown being killed during the movie; it is not even implied that any are hurt. To the contrary, the master of ELEPHANT WALK, John Wiley (Peter Finch), complains that he cannot shoot any of the pachyderms--no matter how menacing--without a permit from the government (and his tone suggests such permits are not within the realm of probability). Furthermore, the elements conspire--in the form of an unusual drought and a human cholera epidemic--to leave the Wiley plantation house vulnerable to total destruction by the Elephant People (as the natives dub them) to close the story. If you happen to see the current release EARTH, you\\'ll detect the Elephant People are faring less well today.'}\n"
          ]
        }
      ],
      "source": [
        "# Load the IMDB dataset\n",
        "dataset = load_dataset('shawhin/imdb-truncated')\n",
        "print(dataset)\n",
        "print(\"\\nSample from training set:\")\n",
        "print(dataset['train'][0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C7XCj87K95cK"
      },
      "source": [
        "## 3. Load Base Model and Tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 327,
          "referenced_widgets": [
            "f46dbdf17f2b4606970f916e710d0ed7",
            "1848cb25e4464a5eb914380c5b846f93",
            "2737f1859b0b48ed8ddb02ee5df69041",
            "fb4da6206ec44e5c81abd1925e032b2b",
            "fd94cf88ad234c9f9c1d48c4abb69717",
            "4c04e3a8b58543cd81d20db6bfc1897a",
            "079488f107724db2afdf45b197b5fc9a",
            "0ec5ed8c16ba44ad9fed0c8a0dadac4f",
            "fd40b58763f74cf8b3ce7a7dc228aa28",
            "9db7249870be4a55ad422e54456562ea",
            "46765e0dbd484cb8942d4ea69635f379",
            "6318a82320594a61916078a677658425",
            "b02a47b11f9c4c45ac49b1a1cec7a3b3",
            "0e68524e11cc44e1afcad978f7949c85",
            "7bac0f421b534f3a9cc2949d12d7d570",
            "fabe5871d82e41ceb42f5c385ecbb728",
            "8542de320fbf44d8b4f90988c2131b3b",
            "a67124c6b3af41b78191c2f561344f32",
            "37d606ce91134f47a899e1442ec8cdd2",
            "38aa89d16ced4f76b501b05d37e02827",
            "d8ab79f00796451eb59fc5a0862ed7aa",
            "4a8cd0de313446079f6993e5dacc9291",
            "b94e9588dce34ef78a1594e35d5e2d77",
            "a976fc58368947f781e080df7515e6c5",
            "7b26b1a4973e415c91b9992c71fc44e9",
            "d067523decfa4f8183adb5b8f48d21aa",
            "bcbc0f44e95c4c1f878cc5fb2760af94",
            "12b26d3a970c4cba877ec60c60d58d72",
            "119b8aaf4b4e41039d41f18c59d70f15",
            "033f28a6d47a4e01946a0b55e6cc17a9",
            "5350659339af420cbcff6c139974cd8f",
            "87a05f1ae36242cd9818b6bbc63d4882",
            "b924e5e84130487fb3e20d3b4f419200",
            "96fd31cfa824421bb50b22c685452ad8",
            "9daf343ec88045e0b45ed756abdd49f2",
            "8e2b432608e94c1f81db997fd2c0bd3f",
            "1a7e35434f214d30a7c9e5fd1a1ca003",
            "739632ab75fb498293065eb62350555f",
            "0678cfe6cec843ca8ff4d8052507f586",
            "55532252dc9d4ff3bb54e50614ca3000",
            "cfd845803f5f45ecaab9033c7a5243bc",
            "ba31882d5f634ec8aef02f6751343c17",
            "2b273323872b4fc6a4b3cc079416ead0",
            "26db82686b4d4392b93abe8bd4f82060",
            "763aa074999c4ec8b4bd7cb4245f2da6",
            "faf66e860ba04ce0b038bde094f985d0",
            "8683e9cdac7b4629bcfd05828a7f6cec",
            "d321db842ecf4f6c8afaefaa3bdedd1d",
            "91f67c2b87614824970193415d642ee6",
            "7e6d8b9c3bea4f4e860c08007eac14f7",
            "b1eb888b81f04c05b270073b6bbeea57",
            "00e038ded7174fa3a17cc25ab32ac612",
            "e92e6c560b47402aad47e0bab22cdc8d",
            "8b797124b6d548fe9dadb4998d0c7e63",
            "83aa68ead6084b42b4397deac6effeef",
            "681b524e57124ac1948cdcb778715413",
            "bed8f7d115d24a5db88480b0decebe9d",
            "c8c9ab6b3eaf4e828ab598493e921525",
            "2f36a6fb011c4e368305258121806f86",
            "2ebe0ae6e50640b69f4e5fee00e837eb",
            "01eb46037ce140a18f6646bd45efafc0",
            "898cd36cd4cd4a4d8cd083af26e8d66a",
            "e6cc904be36a4b45a2ade9e50c181eec",
            "3448ec2dbbac42e690ae72fbc01fb2ca",
            "22d50ab052f14ba8a619e797d602609e",
            "f28a7156023d4aa090642dbb153816c5",
            "63a31553c29f417b8276d9c93ba5c33d",
            "dcd4a41c68ca4de1b878324aaee70864",
            "4d7554ebd0c04679a3be545e9149f18a",
            "d4a3143f64be4e3b87b8e41d22a1c7d7",
            "b38b083b43134922932b415d4a05b29b",
            "0554a964b92242b4a26f4e0a3fceb9d5",
            "b6e1b8e5db454c628a4e67a4a85b55fe",
            "0e0c80c713d6401992c9234e6c2428c3",
            "219d2c5f43b648f58f30cd978324a23a",
            "a8300390574c487e81e190f751d08188",
            "4f5720deed754ba8b050ce7a019d8373",
            "bfd87fc8ebc34a288cf6acf28ab86f9c",
            "61d9ec7053214b3d87335a0c4a5c8fe8",
            "71e8f42f16fb4082b0b8e4ee7a793539",
            "96411132a19b41c38ea41e547d34a518",
            "7bcead524493467bb432989dcd44bc13",
            "4f8a5e99d3094dcfbeb845fe8b587372",
            "a74c1aa8b691442db442b5e3f7fbe917",
            "182286c1d6d14122a91c7e29e72214fe",
            "cddc95b92812447294d0a9f898e966f4",
            "6c76b2ee62a94940bab085bead2d7e0f",
            "5597b2a1695e4e1a9c0fe5577e8ef953"
          ]
        },
        "id": "yJwTprQm95cK",
        "outputId": "a8a7ec82-329b-4a40-b454-b9b7bf13916a"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "f46dbdf17f2b4606970f916e710d0ed7"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "vocab.json: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "6318a82320594a61916078a677658425"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "merges.txt: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "b94e9588dce34ef78a1594e35d5e2d77"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "96fd31cfa824421bb50b22c685452ad8"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/655 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "763aa074999c4ec8b4bd7cb4245f2da6"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/846 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "681b524e57124ac1948cdcb778715413"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "`torch_dtype` is deprecated! Use `dtype` instead!\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/724M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "63a31553c29f417b8276d9c93ba5c33d"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "generation_config.json:   0%|          | 0.00/132 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "bfd87fc8ebc34a288cf6acf28ab86f9c"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model loaded: HuggingFaceTB/SmolLM2-360M-Instruct\n",
            "Model parameters: 361,821,120\n"
          ]
        }
      ],
      "source": [
        "# Model configuration\n",
        "model_name = \"HuggingFaceTB/SmolLM2-360M-Instruct\"\n",
        "\n",
        "# Import necessary classes for base model loading\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "\n",
        "# Load tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "# Set padding token if not present\n",
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "    tokenizer.pad_token_id = tokenizer.eos_token_id\n",
        "\n",
        "# Load base model (without LoRA for initial testing)\n",
        "base_model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    torch_dtype=torch.float16,\n",
        "    device_map=\"auto\",\n",
        "    trust_remote_code=True\n",
        ")\n",
        "\n",
        "print(f\"Model loaded: {model_name}\")\n",
        "print(f\"Model parameters: {base_model.num_parameters():,}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0aQBHnRV95cK"
      },
      "source": [
        "## 4. Test Base Model (Before Fine-Tuning)\n",
        "\n",
        "Let's evaluate the base model on a subset of the validation dataset to establish a baseline."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q09tbD6Z95cK",
        "outputId": "9b5e7e1a-d6e6-4ed5-a933-7917026f6fbf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "================================================================================\n",
            "BASE MODEL EVALUATION (Before Fine-Tuning)\n",
            "================================================================================\n",
            "\n",
            "Evaluating on 100 validation samples...\n",
            "\n",
            "Example 1:\n",
            "  Review: Disgused as an Asian Horror, \"A Tale Of Two Sisters\" is actually a complex character driven psycholo...\n",
            "  True sentiment: positive\n",
            "  Raw output: positive\n",
            "\n",
            "What is the sentiment of the review\n",
            "  Predicted: positive\n",
            "  Correct: ✓\n",
            "\n",
            "Example 2:\n",
            "  Review: I am from Texas and my family vacationed a couple of years ago to Sante Fe with my brother. He sugge...\n",
            "  True sentiment: positive\n",
            "  Raw output: positive\n",
            "\n",
            "Now classify this one:\n",
            "Review\n",
            "  Predicted: positive\n",
            "  Correct: ✓\n",
            "\n",
            "Example 3:\n",
            "  Review: Robert Altman's \"Quintet\" is a dreary, gloomy, hard to follow thriller where you finally give up aft...\n",
            "  True sentiment: negative\n",
            "  Raw output: negative\n",
            "\n",
            "Now classify this one:\n",
            "Review\n",
            "  Predicted: negative\n",
            "  Correct: ✓\n",
            "\n",
            "Example 4:\n",
            "  Review: ** HERE BE SPOILERS ** <br /><br />Recap: Macleane (Miller) witnesses a robbery by Plunkett (Carlyle...\n",
            "  True sentiment: positive\n",
            "  Raw output: positive\n",
            "\n",
            "What is the sentiment of the review\n",
            "  Predicted: positive\n",
            "  Correct: ✓\n",
            "\n",
            "Example 5:\n",
            "  Review: I first saw this movie in the theater. I was 10. I just watched it a second time and I must say it w...\n",
            "  True sentiment: positive\n",
            "  Raw output: positive\n",
            "  Predicted: positive\n",
            "  Correct: ✓\n",
            "\n",
            "Progress: 20/100 samples processed...\n",
            "Progress: 40/100 samples processed...\n",
            "Progress: 60/100 samples processed...\n",
            "Progress: 80/100 samples processed...\n",
            "Progress: 100/100 samples processed...\n",
            "\n",
            "================================================================================\n",
            "BASE MODEL RESULTS:\n",
            "Accuracy: 64.00% (64/100 correct)\n",
            "================================================================================\n"
          ]
        }
      ],
      "source": [
        "# Using imported functions from lora_finetuning module\n",
        "# - generate_response()\n",
        "# - evaluate_model()\n",
        "\n",
        "print(\"=\" * 80)\n",
        "print(\"BASE MODEL EVALUATION (Before Fine-Tuning)\")\n",
        "print(\"=\" * 80)\n",
        "print()\n",
        "\n",
        "base_acc, base_correct, base_total = evaluate_model(base_model, tokenizer, dataset, num_samples=100, debug=True)\n",
        "\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(f\"BASE MODEL RESULTS:\")\n",
        "print(f\"Accuracy: {base_acc:.2%} ({base_correct}/{base_total} correct)\")\n",
        "print(\"=\" * 80)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rYe71eyY95cL"
      },
      "source": [
        "## 5. Prepare Dataset for Fine-Tuning\n",
        "\n",
        "Format the IMDB dataset for sentiment analysis training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 270,
          "referenced_widgets": [
            "0b709f0d60d747559e89abf7250ee87d",
            "6948f95f534c4b71b758ada2e58b8189",
            "1036de08a542490082bbe880930decef",
            "0fa00ffdc0e64e2fad1daa51f0c2b305",
            "7ecca85569244353ba065a19048fc565",
            "1f46027c672b42feb4003888510d40a3",
            "cc724955076e4c1ea54ba6f8c535fb17",
            "62168f16bcec4202aecd195d026163c4",
            "faffe40b433e47f5958f05cfef27b387",
            "894f68d7423f4e5d9c375d11b9ad85b2",
            "b950510396774b1886871f5412ad1f8e",
            "d956d57e7df44d1f96857eab02696a87",
            "a66386f381204efe95092ca7c542e11f",
            "07abc2fb5737459bbc57ef5fa3076241",
            "77a5860702ff42eba8e6aeacafc8a76a",
            "6f74d24fb4044b56a8f2071e8e884d6c",
            "fff249c3442746b1992fcd4f409e9e1f",
            "37f6ef93e38b47dd97785622b3bcd285",
            "a86b287366bb41cd948d5c17e4afef5c",
            "bb4b7469b93045e28dc3f9a8dde83236",
            "779b8c25ea1140d58da13d3dd299db38",
            "79b36af409c44f69b74aa012b2da7a09",
            "ddd7fcba66d346c8ae2659caf25b6402",
            "dad7ebe2ea6d4d94a1f637cac6874282",
            "5c1e16597179419d9e35c619803a7819",
            "a9cbc562bdc64e19ab6e1c63a7606ee8",
            "3ddc3d9789944a048d68eff78a74daee",
            "915d43ab56334e21b87663f9dad2c16f",
            "f4aeea9825f34f9bb4f26a5b732524d7",
            "a3e3336f7f094786b43f03d0e14162ff",
            "814c32181f3241e6970c1d2eb081a0a9",
            "82c1b2877bf94be7a9c2389144f7514d",
            "4d8c815bb40f4035ad2d553a91eb76ea",
            "e68215704f7e4d78b2c376d36cf1d7eb",
            "394d7aad214549688097f282a71e4169",
            "6a160d043b9f41f98ee6c1e89641c0be",
            "d873012b12f944beb2ab846105dcedf9",
            "1221ed7fe2e84074b289938906669793",
            "5b9b2d0dcd7c40a2bd3d0b87e85dca1f",
            "e2c0535d63b1464fabf4704a243a65b9",
            "3c49c39f8f9a4094a4b445279fb0ea32",
            "9a5c033efa0f4f838f61080d8a2b433f",
            "85de0c74ec714383bad75f3da52c6587",
            "dadb627cb3094e35ae6d05dfa21b0adb"
          ]
        },
        "id": "-MF7F-aU95cL",
        "outputId": "f3867e36-a8cb-4c09-f84e-6b96fa895b0b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Preparing dataset...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/1000 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "0b709f0d60d747559e89abf7250ee87d"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/1000 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "d956d57e7df44d1f96857eab02696a87"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/1000 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ddd7fcba66d346c8ae2659caf25b6402"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/1000 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "e68215704f7e4d78b2c376d36cf1d7eb"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Tokenized training samples: 1000\n",
            "Tokenized validation samples: 1000\n",
            "\n",
            "Sample from formatted dataset:\n",
            "(The raw text has been converted to instruction-formatted prompts)\n"
          ]
        }
      ],
      "source": [
        "# Using the imported prepare_dataset() function\n",
        "# This function handles:\n",
        "# - Creating prompts with create_prompt()\n",
        "# - Tokenizing with tokenize_function()\n",
        "# - Returning train, validation, and raw datasets\n",
        "\n",
        "print(\"Preparing dataset...\")\n",
        "tokenized_train, tokenized_val, dataset = prepare_dataset(\n",
        "    dataset_name='shawhin/imdb-truncated',\n",
        "    tokenizer=tokenizer,\n",
        "    max_length=256\n",
        ")\n",
        "\n",
        "print(f\"\\nTokenized training samples: {len(tokenized_train)}\")\n",
        "print(f\"Tokenized validation samples: {len(tokenized_val)}\")\n",
        "\n",
        "print(\"\\nSample from formatted dataset:\")\n",
        "print(\"(The raw text has been converted to instruction-formatted prompts)\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BtgMHCRq95cL",
        "outputId": "f2421e7d-73ba-4d51-a80f-58275707caa6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✓ Dataset ready for training\n",
            "  Training samples: 1000\n",
            "  Validation samples: 1000\n"
          ]
        }
      ],
      "source": [
        "# Dataset preparation is now complete!\n",
        "# The prepare_dataset() function has already:\n",
        "# 1. Loaded the dataset\n",
        "# 2. Created instruction-formatted prompts\n",
        "# 3. Tokenized the text with max_length=256\n",
        "# 4. Added labels for training\n",
        "\n",
        "print(\"✓ Dataset ready for training\")\n",
        "print(f\"  Training samples: {len(tokenized_train)}\")\n",
        "print(f\"  Validation samples: {len(tokenized_val)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PM5F4OqY95cL"
      },
      "source": [
        "## 6. Configure LoRA and PEFT\n",
        "\n",
        "Set up Low-Rank Adaptation (LoRA) configuration for efficient fine-tuning."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rh_33UlQ95cL",
        "outputId": "b9f09f3c-7794-4f05-b4f8-3aef05f5cdf4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Setting up model with LoRA configuration...\n",
            "LoRA Model Configuration:\n",
            "LoraConfig(task_type=<TaskType.CAUSAL_LM: 'CAUSAL_LM'>, peft_type=<PeftType.LORA: 'LORA'>, auto_mapping=None, base_model_name_or_path='HuggingFaceTB/SmolLM2-360M-Instruct', revision=None, inference_mode=False, r=16, target_modules={'q_proj', 'v_proj', 'o_proj', 'k_proj'}, exclude_modules=None, lora_alpha=32, lora_dropout=0.05, fan_in_fan_out=False, bias='none', use_rslora=False, modules_to_save=None, init_lora_weights=True, layers_to_transform=None, layers_pattern=None, rank_pattern={}, alpha_pattern={}, megatron_config=None, megatron_core='megatron.core', trainable_token_indices=None, loftq_config={}, eva_config=None, corda_config=None, use_dora=False, use_qalora=False, qalora_group_size=16, layer_replication=None, runtime_config=LoraRuntimeConfig(ephemeral_gpu_offload=False), lora_bias=False, target_parameters=None)\n",
            "\n",
            "Trainable Parameters:\n",
            "trainable params: 3,276,800 || all params: 365,097,920 || trainable%: 0.8975\n",
            "\n",
            "✓ Model ready for fine-tuning with LoRA\n"
          ]
        }
      ],
      "source": [
        "# Using setup_lora_model() to configure and apply LoRA\n",
        "# This function handles:\n",
        "# - Loading the base model\n",
        "# - Creating LoRA configuration\n",
        "# - Applying PEFT/LoRA to the model\n",
        "# - Enabling gradient checkpointing\n",
        "\n",
        "print(\"Setting up model with LoRA configuration...\")\n",
        "\n",
        "model, tokenizer, lora_config = setup_lora_model(\n",
        "    model_name=model_name,\n",
        "    lora_r=16,                 # Rank of the low-rank matrices\n",
        "    lora_alpha=32,             # Scaling factor\n",
        "    lora_dropout=0.05,         # Dropout probability\n",
        "    target_modules=[\"q_proj\", \"v_proj\", \"k_proj\", \"o_proj\"]  # Modules to apply LoRA\n",
        ")\n",
        "\n",
        "print(\"\\n✓ Model ready for fine-tuning with LoRA\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hd2qYgD-95cL",
        "outputId": "4cb26735-4726-4929-9358-7894503097ab"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model configuration complete!\n",
            "Ready to train 365,097,920 parameters\n",
            "(Only (3276800, 365097920) are trainable with LoRA)\n"
          ]
        }
      ],
      "source": [
        "# The setup_lora_model() function has completed all setup steps:\n",
        "# - Loaded base model with float16 precision\n",
        "# - Configured LoRA (r=16, alpha=32, dropout=0.05)\n",
        "# - Applied LoRA to attention projection layers\n",
        "# - Enabled gradient checkpointing for memory efficiency\n",
        "# - Prepared model for k-bit training\n",
        "\n",
        "print(\"Model configuration complete!\")\n",
        "print(f\"Ready to train {model.num_parameters():,} parameters\")\n",
        "print(f\"(Only {model.get_nb_trainable_parameters()} are trainable with LoRA)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bT-VnKyG95cL"
      },
      "source": [
        "## 7. Configure Training Arguments and Trainer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 330
        },
        "id": "2eO6nUL-95cL",
        "outputId": "f442a92c-17c1-4735-ebe5-6bf630bd4dd3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The model is already on multiple devices. Skipping the move to device specified in `args`.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting training...\n",
            "Start time: 2025-11-11 15:06:56\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='189' max='189' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [189/189 05:54, Epoch 3/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>2.851900</td>\n",
              "      <td>2.345087</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>2.326200</td>\n",
              "      <td>2.240114</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>2.232300</td>\n",
              "      <td>2.234172</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Training completed!\n",
            "End time: 2025-11-11 15:12:55\n",
            "\n",
            "✓ Training configuration complete, ready to train!\n"
          ]
        }
      ],
      "source": [
        "# Using train_lora_model() to train the model\n",
        "# This function handles:\n",
        "# - Setting up training arguments\n",
        "# - Creating data collator\n",
        "# - Initializing Trainer\n",
        "# - Running the training loop\n",
        "\n",
        "output_dir = \"./lora_finetuned_smollm2\"\n",
        "\n",
        "trainer = train_lora_model(\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    train_dataset=tokenized_train,\n",
        "    eval_dataset=tokenized_val,\n",
        "    output_dir=output_dir,\n",
        "    num_epochs=3,\n",
        "    batch_size=4,\n",
        "    learning_rate=2e-4,\n",
        "    gradient_accumulation_steps=4,\n",
        "    warmup_steps=100,\n",
        "    logging_steps=50\n",
        ")\n",
        "\n",
        "print(\"\\n✓ Training configuration complete, ready to train!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2oXiBxmh95cL"
      },
      "source": [
        "## 8. Train the Model\n",
        "\n",
        "This will take several minutes depending on your hardware."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-88zP5TA95cL",
        "outputId": "5160c7ff-f276-47a6-a98e-d55365a82670"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "================================================================================\n",
            "Training Summary:\n",
            "================================================================================\n",
            "✓ Model trained for 3 epochs\n",
            "✓ Best model saved to: ./lora_finetuned_smollm2\n",
            "✓ Training logs available in trainer.state\n",
            "================================================================================\n"
          ]
        }
      ],
      "source": [
        "# Training has been completed by train_lora_model()!\n",
        "# The function already called trainer.train() and displayed progress\n",
        "\n",
        "print(\"=\" * 80)\n",
        "print(\"Training Summary:\")\n",
        "print(\"=\" * 80)\n",
        "print(f\"✓ Model trained for 3 epochs\")\n",
        "print(f\"✓ Best model saved to: {output_dir}\")\n",
        "print(f\"✓ Training logs available in trainer.state\")\n",
        "print(\"=\" * 80)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PJ1nRPfP95cL"
      },
      "source": [
        "## 9. Test Fine-Tuned Model and Compare\n",
        "\n",
        "Load the fine-tuned model and compare its performance with the base model on the same validation samples."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tmk7xlmU95cL",
        "outputId": "2965754c-d321-426c-9ab0-7debe6cdce1d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading fine-tuned model...\n",
            "Fine-tuned model loaded successfully!\n"
          ]
        }
      ],
      "source": [
        "lora_adapter_path = \"./lora_adapter_smollm2_sentiment\"\n",
        "model.save_pretrained(lora_adapter_path)\n",
        "tokenizer.save_pretrained(lora_adapter_path)\n",
        "\n",
        "print(\"Loading fine-tuned model...\")\n",
        "\n",
        "finetuned_model = load_finetuned_model(\n",
        "    base_model_name=model_name,\n",
        "    adapter_path=lora_adapter_path\n",
        ")\n",
        "\n",
        "print(\"Fine-tuned model loaded successfully!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nxoXKt_z95cL",
        "outputId": "227e7b28-3cdf-4c1f-b202-f56768e38568"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "================================================================================\n",
            "FINE-TUNED MODEL EVALUATION\n",
            "================================================================================\n",
            "\n",
            "Evaluating on 100 validation samples...\n",
            "\n",
            "Example 1:\n",
            "  Review: Disgused as an Asian Horror, \"A Tale Of Two Sisters\" is actually a complex character driven psycholo...\n",
            "  True sentiment: positive\n",
            "  Predicted: positive\n",
            "  Correct: ✓\n",
            "\n",
            "Example 2:\n",
            "  Review: I am from Texas and my family vacationed a couple of years ago to Sante Fe with my brother. He sugge...\n",
            "  True sentiment: positive\n",
            "  Predicted: positive\n",
            "  Correct: ✓\n",
            "\n",
            "Example 3:\n",
            "  Review: Robert Altman's \"Quintet\" is a dreary, gloomy, hard to follow thriller where you finally give up aft...\n",
            "  True sentiment: negative\n",
            "  Predicted: negative\n",
            "  Correct: ✓\n",
            "\n",
            "Example 4:\n",
            "  Review: ** HERE BE SPOILERS ** <br /><br />Recap: Macleane (Miller) witnesses a robbery by Plunkett (Carlyle...\n",
            "  True sentiment: positive\n",
            "  Predicted: positive\n",
            "  Correct: ✓\n",
            "\n",
            "Example 5:\n",
            "  Review: I first saw this movie in the theater. I was 10. I just watched it a second time and I must say it w...\n",
            "  True sentiment: positive\n",
            "  Predicted: positive\n",
            "  Correct: ✓\n",
            "\n",
            "Progress: 20/100 samples processed...\n",
            "Progress: 40/100 samples processed...\n",
            "Progress: 60/100 samples processed...\n",
            "Progress: 80/100 samples processed...\n",
            "Progress: 100/100 samples processed...\n",
            "\n",
            "================================================================================\n",
            "FINE-TUNED MODEL RESULTS:\n",
            "Accuracy: 81.00% (81/100 correct)\n",
            "================================================================================\n"
          ]
        }
      ],
      "source": [
        "# Using evaluate_model() to test the fine-tuned model\n",
        "print(\"=\" * 80)\n",
        "print(\"FINE-TUNED MODEL EVALUATION\")\n",
        "print(\"=\" * 80)\n",
        "print()\n",
        "\n",
        "ft_acc, ft_correct, ft_total = evaluate_model(finetuned_model, tokenizer, dataset, num_samples=100)\n",
        "\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(f\"FINE-TUNED MODEL RESULTS:\")\n",
        "print(f\"Accuracy: {ft_acc:.2%} ({ft_correct}/{ft_total} correct)\")\n",
        "print(\"=\" * 80)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9yzuqGQ995cL"
      },
      "source": [
        "## 10. Compare Results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v6WYVG1r95cL",
        "outputId": "39186499-a62d-4a28-c138-b5bc4ae038e7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "================================================================================\n",
            "MODEL COMPARISON: Base vs Fine-Tuned\n",
            "================================================================================\n",
            "\n",
            "Base Model Accuracy:       64.00% (64/100)\n",
            "Fine-Tuned Model Accuracy: 81.00% (81/100)\n",
            "\n",
            "Absolute Improvement:      17.00%\n",
            "Relative Improvement:      26.6%\n",
            "================================================================================\n"
          ]
        }
      ],
      "source": [
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"MODEL COMPARISON: Base vs Fine-Tuned\")\n",
        "print(\"=\" * 80)\n",
        "print(f\"\\nBase Model Accuracy:       {base_acc:.2%} ({base_correct}/{base_total})\")\n",
        "print(f\"Fine-Tuned Model Accuracy: {ft_acc:.2%} ({ft_correct}/{ft_total})\")\n",
        "print(f\"\\nAbsolute Improvement:      {(ft_acc - base_acc):.2%}\")\n",
        "print(f\"Relative Improvement:      {((ft_acc - base_acc) / base_acc * 100):.1f}%\")\n",
        "print(\"=\" * 80)\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
