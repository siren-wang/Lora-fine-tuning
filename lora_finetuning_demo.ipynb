{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hErMr4PenZIo"
      },
      "source": [
        "# LoRA Fine-Tuning Demo with SmolLM2-360M\n",
        "\n",
        "This notebook demonstrates:\n",
        "1. Testing the base model on sample inputs\n",
        "2. Fine-tuning using PEFT/LoRA on IMDB sentiment dataset\n",
        "3. Comparing base vs fine-tuned model performance\n",
        "4. Saving and loading the fine-tuned model\n",
        "\n",
        "**Base Model:** HuggingFaceTB/SmolLM2-360M-Instruct  \n",
        "**Dataset:** shawhin/imdb-truncated (1000 train, 1000 validation samples)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7UwiIowQnZIq"
      },
      "source": [
        "## 1. Setup and Installation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "rAXg13sHnZIq",
        "outputId": "0d1f412a-5f0f-474b-bddb-1422ee96ffd1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m59.4/59.4 MB\u001b[0m \u001b[31m15.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m462.8/462.8 kB\u001b[0m \u001b[31m17.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "# Install required packages\n",
        "!pip install -q transformers datasets peft accelerate bitsandbytes trl torch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "akGNmXuQnZIr"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, TrainingArguments, Trainer, DataCollatorForLanguageModeling\n",
        "from datasets import load_dataset\n",
        "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training, TaskType\n",
        "import numpy as np\n",
        "from datetime import datetime\n",
        "import json\n",
        "\n",
        "# Set random seeds for reproducibility\n",
        "torch.manual_seed(42)\n",
        "np.random.seed(42)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yxL9CCkenZIr"
      },
      "source": [
        "## 2. Load Dataset and Inspect"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bFfaBDA3nZIr"
      },
      "outputs": [],
      "source": [
        "# Load the IMDB dataset\n",
        "dataset = load_dataset('shawhin/imdb-truncated')\n",
        "print(dataset)\n",
        "print(\"\\nSample from training set:\")\n",
        "print(dataset['train'][0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V-NjTvNGnZIr"
      },
      "source": [
        "## 3. Load Base Model and Tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hfmjvCMvnZIs"
      },
      "outputs": [],
      "source": [
        "# Model configuration\n",
        "model_name = \"HuggingFaceTB/SmolLM2-360M-Instruct\"\n",
        "\n",
        "# Load tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "# Set padding token if not present\n",
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "    tokenizer.pad_token_id = tokenizer.eos_token_id\n",
        "\n",
        "# Load base model\n",
        "base_model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    torch_dtype=torch.float16,\n",
        "    device_map=\"auto\",\n",
        "    trust_remote_code=True\n",
        ")\n",
        "\n",
        "print(f\"Model loaded: {model_name}\")\n",
        "print(f\"Model parameters: {base_model.num_parameters():,}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zjLLfkTYnZIs"
      },
      "source": [
        "## 4. Test Base Model (Before Fine-Tuning)\n",
        "\n",
        "Let's test the base model on some sentiment analysis tasks to establish a baseline."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ddJTT1NTnZIs"
      },
      "outputs": [],
      "source": [
        "# Test prompts - NOT from training data\n",
        "test_prompts = [\n",
        "    \"Review: This restaurant exceeded all my expectations. The food was delicious and the service was impeccable.\\nSentiment:\",\n",
        "    \"Review: Terrible experience. The product broke after one day and customer service was unhelpful.\\nSentiment:\",\n",
        "    \"Review: Average movie, nothing special but not terrible either. Worth watching if you have time.\\nSentiment:\",\n",
        "    \"Review: Absolutely loved this book! Couldn't put it down, read it in one sitting.\\nSentiment:\",\n",
        "    \"Review: Waste of money. Poor quality and doesn't work as advertised.\\nSentiment:\"\n",
        "]\n",
        "\n",
        "def generate_response(model, prompt, max_new_tokens=50):\n",
        "    \"\"\"Generate response from model\"\"\"\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = model.generate(\n",
        "            **inputs,\n",
        "            max_new_tokens=max_new_tokens,\n",
        "            temperature=0.7,\n",
        "            do_sample=True,\n",
        "            pad_token_id=tokenizer.pad_token_id\n",
        "        )\n",
        "\n",
        "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "    return response\n",
        "\n",
        "print(\"=\" * 80)\n",
        "print(\"BASE MODEL OUTPUTS (Before Fine-Tuning)\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "base_outputs = []\n",
        "for i, prompt in enumerate(test_prompts, 1):\n",
        "    print(f\"\\n--- Test {i} ---\")\n",
        "    print(f\"Prompt: {prompt}\")\n",
        "    output = generate_response(base_model, prompt)\n",
        "    base_outputs.append(output)\n",
        "    print(f\"Output: {output}\")\n",
        "    print(\"-\" * 80)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E1vylp6xnZIt"
      },
      "source": [
        "## 5. Prepare Dataset for Fine-Tuning\n",
        "\n",
        "Format the IMDB dataset for sentiment analysis training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KbRxS0PAnZIt"
      },
      "outputs": [],
      "source": [
        "def create_prompt(example):\n",
        "    \"\"\"Create instruction-formatted prompt for sentiment analysis\"\"\"\n",
        "    sentiment = \"positive\" if example['label'] == 1 else \"negative\"\n",
        "\n",
        "    # Instruction format\n",
        "    prompt = f\"Review: {example['text']}\\nSentiment: {sentiment}\"\n",
        "\n",
        "    return {\"text\": prompt}\n",
        "\n",
        "# Format datasets\n",
        "formatted_train = dataset['train'].map(create_prompt, remove_columns=['label'])\n",
        "formatted_val = dataset['validation'].map(create_prompt, remove_columns=['label'])\n",
        "\n",
        "print(\"Sample formatted training example:\")\n",
        "print(formatted_train[0]['text'][:200] + \"...\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ipMnJT3BnZIt"
      },
      "outputs": [],
      "source": [
        "def tokenize_function(examples):\n",
        "    \"\"\"Tokenize the text data\"\"\"\n",
        "    tokenized = tokenizer(\n",
        "        examples['text'],\n",
        "        truncation=True,\n",
        "        max_length=256,\n",
        "        padding=\"max_length\",\n",
        "        return_tensors=None\n",
        "    )\n",
        "    tokenized[\"labels\"] = tokenized[\"input_ids\"].copy()\n",
        "    return tokenized\n",
        "\n",
        "# Tokenize datasets\n",
        "tokenized_train = formatted_train.map(\n",
        "    tokenize_function,\n",
        "    batched=True,\n",
        "    remove_columns=['text']\n",
        ")\n",
        "\n",
        "tokenized_val = formatted_val.map(\n",
        "    tokenize_function,\n",
        "    batched=True,\n",
        "    remove_columns=['text']\n",
        ")\n",
        "\n",
        "print(f\"\\nTokenized training samples: {len(tokenized_train)}\")\n",
        "print(f\"Tokenized validation samples: {len(tokenized_val)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YPCxVAQ4nZIt"
      },
      "source": [
        "## 6. Configure LoRA and PEFT\n",
        "\n",
        "Set up Low-Rank Adaptation (LoRA) configuration for efficient fine-tuning."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zrVTrl-VnZIt"
      },
      "outputs": [],
      "source": [
        "# LoRA configuration\n",
        "lora_config = LoraConfig(\n",
        "    r=16,                          # Rank of the low-rank matrices\n",
        "    lora_alpha=32,                 # Scaling factor\n",
        "    target_modules=[\"q_proj\", \"v_proj\", \"k_proj\", \"o_proj\"],  # Modules to apply LoRA\n",
        "    lora_dropout=0.05,             # Dropout probability\n",
        "    bias=\"none\",                   # Bias training strategy\n",
        "    task_type=TaskType.CAUSAL_LM   # Task type\n",
        ")\n",
        "\n",
        "print(\"LoRA Configuration:\")\n",
        "print(lora_config)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JVBDcbzdnZIt"
      },
      "outputs": [],
      "source": [
        "# Create a fresh model for training\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    torch_dtype=torch.float16,\n",
        "    device_map=\"auto\",\n",
        "    trust_remote_code=True\n",
        ")\n",
        "\n",
        "# Enable gradient checkpointing for memory efficiency\n",
        "model.gradient_checkpointing_enable()\n",
        "\n",
        "# Prepare model for training\n",
        "model = prepare_model_for_kbit_training(model)\n",
        "\n",
        "# Apply LoRA\n",
        "model = get_peft_model(model, lora_config)\n",
        "\n",
        "# Print trainable parameters\n",
        "model.print_trainable_parameters()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6GYt3TVznZIt"
      },
      "source": [
        "## 7. Configure Training Arguments and Trainer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YCQTOvyPnZIt"
      },
      "outputs": [],
      "source": [
        "# Output directory for checkpoints\n",
        "output_dir = \"./lora_finetuned_smollm2\"\n",
        "\n",
        "# Training arguments\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=output_dir,\n",
        "    num_train_epochs=3,\n",
        "    per_device_train_batch_size=4,\n",
        "    per_device_eval_batch_size=4,\n",
        "    gradient_accumulation_steps=4,\n",
        "    learning_rate=2e-4,\n",
        "    warmup_steps=100,\n",
        "    logging_steps=50,\n",
        "    save_strategy=\"epoch\",\n",
        "    evaluation_strategy=\"epoch\",\n",
        "    fp16=True,\n",
        "    push_to_hub=False,\n",
        "    report_to=\"none\",\n",
        "    load_best_model_at_end=True,\n",
        "    metric_for_best_model=\"eval_loss\",\n",
        "    greater_is_better=False,\n",
        ")\n",
        "\n",
        "# Data collator\n",
        "data_collator = DataCollatorForLanguageModeling(\n",
        "    tokenizer=tokenizer,\n",
        "    mlm=False\n",
        ")\n",
        "\n",
        "# Initialize Trainer\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_train,\n",
        "    eval_dataset=tokenized_val,\n",
        "    data_collator=data_collator,\n",
        ")\n",
        "\n",
        "print(\"Trainer initialized successfully!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jLi9e518nZIt"
      },
      "source": [
        "## 8. Train the Model\n",
        "\n",
        "This will take several minutes depending on your hardware."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZJESaBoinZIt"
      },
      "outputs": [],
      "source": [
        "print(\"Starting training...\")\n",
        "print(f\"Start time: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
        "\n",
        "# Train the model\n",
        "trainer.train()\n",
        "\n",
        "print(f\"\\nTraining completed!\")\n",
        "print(f\"End time: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oJd_0sQGnZIt"
      },
      "source": [
        "## 9. Save the Fine-Tuned Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XM_e4-UmnZIt"
      },
      "outputs": [],
      "source": [
        "# Save the LoRA adapter\n",
        "lora_adapter_path = \"./lora_adapter_smollm2_sentiment\"\n",
        "model.save_pretrained(lora_adapter_path)\n",
        "tokenizer.save_pretrained(lora_adapter_path)\n",
        "\n",
        "print(f\"LoRA adapter saved to: {lora_adapter_path}\")\n",
        "\n",
        "# Save training metadata\n",
        "metadata = {\n",
        "    \"base_model\": model_name,\n",
        "    \"dataset\": \"shawhin/imdb-truncated\",\n",
        "    \"train_samples\": len(tokenized_train),\n",
        "    \"val_samples\": len(tokenized_val),\n",
        "    \"lora_config\": {\n",
        "        \"r\": lora_config.r,\n",
        "        \"lora_alpha\": lora_config.lora_alpha,\n",
        "        \"target_modules\": lora_config.target_modules,\n",
        "        \"lora_dropout\": lora_config.lora_dropout\n",
        "    },\n",
        "    \"training_args\": {\n",
        "        \"num_epochs\": training_args.num_train_epochs,\n",
        "        \"learning_rate\": training_args.learning_rate,\n",
        "        \"batch_size\": training_args.per_device_train_batch_size\n",
        "    },\n",
        "    \"timestamp\": datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
        "}\n",
        "\n",
        "with open(f\"{lora_adapter_path}/training_metadata.json\", \"w\") as f:\n",
        "    json.dump(metadata, f, indent=2)\n",
        "\n",
        "print(\"Training metadata saved!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0zX5lu3GnZIt"
      },
      "source": [
        "## 10. Test Fine-Tuned Model and Compare\n",
        "\n",
        "Load the fine-tuned model and compare its performance with the base model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xzuUO4lknZIu"
      },
      "outputs": [],
      "source": [
        "from peft import PeftModel\n",
        "\n",
        "# Load base model again (fresh)\n",
        "base_model_test = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    torch_dtype=torch.float16,\n",
        "    device_map=\"auto\",\n",
        "    trust_remote_code=True\n",
        ")\n",
        "\n",
        "# Load fine-tuned model (base + LoRA adapter)\n",
        "finetuned_model = PeftModel.from_pretrained(\n",
        "    base_model_test,\n",
        "    lora_adapter_path\n",
        ")\n",
        "\n",
        "print(\"Fine-tuned model loaded successfully!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JKA9cfIBnZIu"
      },
      "outputs": [],
      "source": [
        "print(\"=\" * 80)\n",
        "print(\"MODEL COMPARISON: Base vs Fine-Tuned\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "for i, prompt in enumerate(test_prompts, 1):\n",
        "    print(f\"\\n{'='*80}\")\n",
        "    print(f\"Test {i}\")\n",
        "    print(f\"{'='*80}\")\n",
        "    print(f\"Prompt: {prompt}\\n\")\n",
        "\n",
        "    # Base model output\n",
        "    print(\"BASE MODEL OUTPUT:\")\n",
        "    base_output = generate_response(base_model_test, prompt)\n",
        "    print(base_output)\n",
        "\n",
        "    print(\"\\n\" + \"-\" * 80 + \"\\n\")\n",
        "\n",
        "    # Fine-tuned model output\n",
        "    print(\"FINE-TUNED MODEL OUTPUT:\")\n",
        "    finetuned_output = generate_response(finetuned_model, prompt)\n",
        "    print(finetuned_output)\n",
        "\n",
        "    print(\"\\n\" + \"=\" * 80)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jNXC9V5wnZIu"
      },
      "source": [
        "## 11. Quantitative Evaluation (Optional)\n",
        "\n",
        "Evaluate both models on the validation set to get numerical metrics."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jMmg2MrrnZIu"
      },
      "outputs": [],
      "source": [
        "def evaluate_sentiment_accuracy(model, test_dataset, num_samples=100):\n",
        "    \"\"\"Evaluate model accuracy on sentiment classification\"\"\"\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    # Take a subset of validation data that wasn't in training\n",
        "    samples = test_dataset.select(range(min(num_samples, len(test_dataset))))\n",
        "\n",
        "    for sample in samples:\n",
        "        # Create prompt without answer\n",
        "        review_text = dataset['validation'][total]['text']\n",
        "        true_label = dataset['validation'][total]['label']\n",
        "        true_sentiment = \"positive\" if true_label == 1 else \"negative\"\n",
        "\n",
        "        prompt = f\"Review: {review_text}\\nSentiment:\"\n",
        "\n",
        "        # Generate prediction\n",
        "        output = generate_response(model, prompt, max_new_tokens=10)\n",
        "\n",
        "        # Extract predicted sentiment\n",
        "        output_lower = output.lower()\n",
        "        if \"positive\" in output_lower and \"negative\" not in output_lower:\n",
        "            predicted_sentiment = \"positive\"\n",
        "        elif \"negative\" in output_lower and \"positive\" not in output_lower:\n",
        "            predicted_sentiment = \"negative\"\n",
        "        else:\n",
        "            predicted_sentiment = None\n",
        "\n",
        "        if predicted_sentiment == true_sentiment:\n",
        "            correct += 1\n",
        "\n",
        "        total += 1\n",
        "\n",
        "        if total % 20 == 0:\n",
        "            print(f\"Processed {total}/{num_samples} samples...\")\n",
        "\n",
        "    accuracy = correct / total if total > 0 else 0\n",
        "    return accuracy, correct, total\n",
        "\n",
        "print(\"Evaluating Base Model...\")\n",
        "base_acc, base_correct, base_total = evaluate_sentiment_accuracy(base_model_test, tokenized_val, num_samples=50)\n",
        "\n",
        "print(\"\\nEvaluating Fine-Tuned Model...\")\n",
        "ft_acc, ft_correct, ft_total = evaluate_sentiment_accuracy(finetuned_model, tokenized_val, num_samples=50)\n",
        "\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"EVALUATION RESULTS\")\n",
        "print(\"=\" * 80)\n",
        "print(f\"Base Model Accuracy: {base_acc:.2%} ({base_correct}/{base_total})\")\n",
        "print(f\"Fine-Tuned Model Accuracy: {ft_acc:.2%} ({ft_correct}/{ft_total})\")\n",
        "print(f\"Improvement: {(ft_acc - base_acc):.2%}\")\n",
        "print(\"=\" * 80)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bAvtAQk2nZIu"
      },
      "source": [
        "## 12. Summary and Next Steps\n",
        "\n",
        "### What we accomplished:\n",
        "1. ✅ Tested the base SmolLM2-360M model on sentiment analysis\n",
        "2. ✅ Fine-tuned using LoRA/PEFT on 1000 IMDB reviews\n",
        "3. ✅ Compared base vs fine-tuned model performance\n",
        "4. ✅ Saved the LoRA adapter for future use\n",
        "\n",
        "### Key Takeaways:\n",
        "- LoRA allows efficient fine-tuning with minimal trainable parameters\n",
        "- The fine-tuned model should show improved sentiment classification\n",
        "- Test prompts were kept separate from training data to ensure fair evaluation\n",
        "\n",
        "### To use the fine-tuned model later:\n",
        "```python\n",
        "from peft import PeftModel\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "\n",
        "base_model = AutoModelForCausalLM.from_pretrained(\"HuggingFaceTB/SmolLM2-360M-Instruct\")\n",
        "model = PeftModel.from_pretrained(base_model, \"./lora_adapter_smollm2_sentiment\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"./lora_adapter_smollm2_sentiment\")\n",
        "```"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}